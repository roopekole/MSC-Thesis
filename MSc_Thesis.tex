%% Final edits by Jussi Kangasharju and Pirjo Moen
%% This file is modified by Veli Mäkinen from HY_fysiikka_LuKtemplate.tex authored by Roope Halonen ja Tomi Vainio.
%% Some text is also inherited from engl_malli.tex by Kutvonen, Erkiö, Mäkelä, Verkamo, Kurhila, and Nykänen.


% STEP 1: Choose oneside or twoside
\documentclass[english,twoside,openright]{UH_DS_MSc}
%finnish,swedish

%\usepackage[utf8]{inputenc} % For UTF8 support. Use UTF8 when saving your file.
\usepackage{lmodern} % Font package
\usepackage{textcomp} % Package for special symbols
\usepackage[pdftex]{color, graphicx} % For pdf output and jpg/png graphics
\usepackage[pdftex, plainpages=false]{hyperref} % For hyperlinks and pdf metadata
\usepackage{fancyhdr} % For nicer page headers
\usepackage{tikz} % For making vector graphics (hard to learn but powerful)
%\usepackage{wrapfig} % For nice text-wrapping figures (use at own discretion)
\usepackage{amsmath, amssymb} % For better math
%\usepackage[square]{natbib} % For bibliography
\usepackage[footnotesize,bf]{caption} % For more control over figure captions
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage[titletoc]{appendix}

\onehalfspacing %line spacing
%\singlespacing
%\doublespacing

%\fussy 
\sloppy % sloppy and fussy commands can be used to avoid overlong text lines

% STEP 2:
% Set up all the information for the title page and the abstract form.
% Replace parameters with your information.
\title{Transformers are Zero-Shot / Domain Learners for Technical Analysis}
\author{Roope Kolehmainen}
\date{\today}
\prof{Professor Teemu Roos}
\censors{Professor A}{Dr. B}{}
\keywords{layout, summary, list of references}
\depositeplace{}
\additionalinformation{}


\classification{\protect{\\
\ Computing methodologies $\rightarrow$ Machine learning $\rightarrow$ Machine learning applications $\rightarrow$ Forecasting\\
\ Applied computing $\rightarrow$ Business $\rightarrow$ Financial engineering\\
\ Computing methodologies $\rightarrow$ Natural language processing $\rightarrow$ Language models\\
}}

% if you want to quote someone special. You can comment this line and there will be nothing on the document.
%\quoting{Bachelor's degrees make pretty good placemats if you get them laminated.}{Jeph Jacques} 


% OPTIONAL STEP: Set up properties and metadata for the pdf file that pdfLaTeX makes.
% But you don't really need to do this unless you want to.
\hypersetup{
    %bookmarks=true,         % show bookmarks bar first?
    unicode=true,           % to show non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={},            % title
    pdfauthor={},           % author
    pdfsubject={},          % subject of the document
    pdfcreator={},          % creator of the document
    pdfproducer={pdfLaTeX}, % producer of the document
    pdfkeywords={something} {something else}, % list of keywords for
    pdfnewwindow=true,      % links in new window
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=black,        % color of internal links
    citecolor=black,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

\begin{document}

% Generate title page.
\maketitle

% STEP 3:
% Write your abstract (of course you really do this last).
% You can make several abstract pages (if you want it in different languages),
% but you should also then redefine some of the above parameters in the proper
% language as well, in between the abstract definitions.

\begin{abstract}
This paper investigates the application of transformer-based language models to the domain of financial time series forecasting, specifically focusing on price prediction through technical analysis. While large language models (LLMs) such as GPT have demonstrated exceptional performance in natural language processing tasks, their potential in financial modeling - particularly in learning and predicting chart-based trading patterns - remains underexplored.

The study first outlines the foundational principles of technical analysis and its role in modeling price dynamics in financial markets. It then introduces the transformer architecture, highlighting its core components, self-attention, multi-head attention, and autoregressive modeling, explains how these enable the detection of long-range, non-linear patterns within sequential data. The research evaluates whether transformers can internalize the temporal structures and heuristics that underlie technical trading strategies.

This work contributes to a novel intersection of deep learning and financial engineering, testing the hypothesis that models originally developed for language can meaningfully emulate or enhance rule-based financial analysis.
\end{abstract}

% Place ToC
\mytableofcontents

\mynomenclature

% -----------------------------------------------------------------------------------
% STEP 4: Write the thesis.
% Your actual text starts here. You shouldn't mess with the code above the line except
% to change the parameters. Removing the abstract and ToC commands will mess up stuff.
\chapter{Introduction}

Technical analysis posits that, at least in the short term, the pricing of securities does not strictly follow a stochastic or purely random process. Instead, it suggests that price movements are influenced by investor behavior, which can give rise to identifiable patterns and trends in historical price data. The objective of technical trading is to detect and exploit these patterns to inform profitable trading decisions.

In contrast to fundamental analysis, which relies on indicators such as a firm's financial performance, macroeconomic data, or asset-based valuation, technical analysis focuses exclusively on signals derived from past market behavior. This focus on sequential price dependencies aligns conceptually with the autoregressive modeling paradigm found in contemporary natural language processing (NLP), where future tokens are predicted based on sequences of preceding inputs. Transformer-based models, in particular, have demonstrated exceptional capability in capturing structural patterns in sequential data.

This parallel raises a key research question: \textit{Can transformer architectures, which excel at modeling complex syntactic and semantic structures in language, also be applied to financial time series to extract and forecast meaningful patterns in market behavior}?

Technical analysis provides a compelling testbed for this question. It encompasses a broad range of trading strategies, from intraday high-frequency trading to longer-horizon approaches like momentum and swing trading, all of which depend on identifying repeatable patterns in price movements. Forecasting tasks in this domain can be framed as binary or multiclass classification problems (e.g., up / down movements or buy / hold / sell actions), as well as point-estimation tasks such as trend direction or volatility forecasting.

Moreover, financial markets differ significantly across asset classes and timeframes, requiring tailored modeling strategies. These design decisions include choosing between encoder-decoder versus decoder-only transformer architectures, selecting data granularity (e.g., high-frequency vs. daily), and determining feature sets (e.g., OHLC - Open, High, Low, Close - data, technical indicators).

To explore the applicability of transformer models in this context, this study adopts a dual approach. First, it investigates zero-shot inference using general-purpose large language models (LLMs) such as LLaMA, which have not been fine-tuned on financial data. These models are prompted to analyze market behavior based solely on pre-trained world knowledge and reasoning capabilities. Second, the study develops a domain-specific transformer model trained directly on historical price sequences to learn market-specific dynamics for forecasting tasks.

By comparing these two paradigms - zero-shot generalization versus supervised, task-specific learning - and benchmarking them against traditional baselines (e.g., ARIMA or LSTM), this study seeks to evaluate the strengths, limitations, and practical utility of transformer-based architectures in modeling and predicting patterns in financial time series.

RESULT PLACEHOLDER: Results from this study indicate that [brief description of result; e.g., domain-specific transformer models outperform baseline approaches in certain classification tasks]. Notably, the [model name or approach] achieved [X] \% accuracy / F1-score / Sharpe ratio on [task], surpassing [baseline model] by a margin of [Y]\%. Conversely, the zero-shot inference using general-purpose large language models showed [brief insight e.g., limited quantitative performance but some qualitative reasoning ability]. These findings suggest that [core insight e.g., transformer models can capture meaningful temporal dependencies in financial time series when trained in-domain], but also highlight [a limitation or nuance].

\chapter{Background and Related Work}
Despite the explosive growth in both financial machine learning (ML) and natural language processing (NLP), the direct application of transformer-based large language models (LLMs) to technical analysis for stock prediction remains largely unexplored. Historically, financial forecasting models have relied heavily on statistical and deep learning techniques such as ARIMA, LSTM, GRU, and CNN architectures to model price series. These methods emphasize pattern recognition and temporal dependencies in numerical data, forming the core of traditional technical analysis approaches.

Conversely, transformer-based LLMs like GPT, BERT, and their financial-domain variants (e.g., FinBERT, BloombergGPT) have primarily been utilized for text-based analysis, including sentiment extraction, earnings call interpretation, and market news classification. While some hybrid applications integrate text sentiment with price data, these typically do not utilize LLMs to process raw time series in a manner akin to classical technical charting.

To our knowledge, no prior work has directly leveraged transformer-based LLMs to perform technical-style price movement forecasting - that is, using LLMs to identify and act upon patterns such as breakouts, reversals, or momentum shifts directly from price sequences, candlestick patterns, or volume indicators. This research therefore occupies a novel intersection between the interpretability of traditional technical analysis and the pattern abstraction capacity of LLMs.

Although a few early-stage studies (e.g., StockTime, ElliottAgents) suggest that transformers can capture market structure when fine-tuned on historical charts, these remain exceptions and do not generalize to LLM-based prediction in a traditional technical analysis framework. Our study aims to bridge this gap by exploring whether general-purpose LLMs can emulate or enhance rule-based trading logic, offering a new lens through which to view market dynamics.

\section{Technical Analysis in Financial Markets}

Technical analysis (TA) refers to the forecasting of future price movements in financial markets through the examination of historical market data, primarily price and volume. In contrast to fundamental analysis, which seeks to determine a security's intrinsic value using financial statements or macroeconomic indicators, TA is rooted in the belief that prices already reflect all publicly available information (consistent with the \textit{semi-strong form} of the Efficient Market Hypothesis, EMH). It further assumes that investor psychology and collective market behavior produce identifiable, repetitive price patterns over time \cite{murphy1999}.

TA is implemented through a wide variety of trading strategies suited to different investment horizons:

\begin{itemize}
    \item \textbf{Momentum trading} identifies assets with strong upward or downward trends and follows the trend direction, supported by evidence from \cite{jegadeesh1993} showing persistence in short-term return patterns.
    \item \textbf{Swing trading} targets price movements over several days or weeks, using chart patterns and oscillators to identify temporary fluctuations within broader trends.
    \item \textbf{Mean reversion strategies} are based on the premise that prices tend to revert to a historical mean, and are widely used in statistical arbitrage and high-frequency trading (HFT) contexts.
    \item \textbf{Intraday and high-frequency trading (HFT)} applies TA principles at ultra-short intervals - minutes or even milliseconds - aiming to exploit micro-inefficiencies and liquidity gaps in real-time markets.
\end{itemize}

Technical analysis employs a variety of analytical tools designed to identify patterns and predict market movements. These tools generally fall into three main methodological classes, each reflecting a distinct approach to interpreting price and volume data:

\begin{itemize}
    \item \textbf{Chart patterns}, such as head-and-shoulders, triangles, and double tops/bottoms, offer visual cues that practitioners interpret as signals of trend continuation or reversal.
    \item \textbf{Indicators and oscillators}, including moving averages, the Moving Average Convergence Divergence (MACD), the Relative Strength Index (RSI), and Bollinger Bands, provide smoothed, quantitative insights into price dynamics, trend strength, and market conditions \cite{wilder1978, achelis2001}.
    \item \textbf{Statistical models}, such as the Autoregressive Integrated Moving Average (ARIMA) model \cite{dickey1979} and the Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model \cite{bollerslev1986}, aim to model autocorrelation, volatility clustering, and non-stationary behavior in asset returns.
\end{itemize}

Despite its widespread use among practitioners, TA has faced sustained criticism in academic finance, particularly from proponents of the EMH. According to EMH, particularly in its strong and semi-strong forms \cite{fama1970}, all relevant information is already reflected in asset prices, making it impossible to systematically outperform the market through pattern recognition or historical price analysis. From this perspective, TA is no more effective than random guessing, and any perceived patterns are likely artifacts of data mining or overfitting.

Critics also argue that many technical patterns lack formal theoretical grounding, are not rigorously defined, and are subject to confirmation bias in interpretation. Empirical studies that fail to adjust for transaction costs, slippage, and overfitting in backtesting further challenge the validity of many TA-based strategies.

However, TA has also been defended on both empirical and behavioral grounds. For example, \cite{brock1992} demonstrated that simple moving average and trading range breakout rules generated significant excess returns in historical simulations. Behavioral finance adds further nuance by suggesting that markets are not always rational; cognitive biases, herd behavior, and overreactions may indeed cause patterns that TA can exploit at least temporarily.

Recent work by \cite{ni2024} shows that TA continues to play a practical role in energy and financial markets, especially for short-term speculation and risk management. Moreover, the rise of algorithmic and quantitative trading has enabled more rigorous and systematic implementations of TA, increasingly enhanced by data-driven methods such as machine learning \cite{zhang2023}.


\section{Machine Learning in Financial Forecasting}

The increasing availability of financial data and advances in computational power have accelerated the adoption of machine learning (ML) techniques in financial forecasting. While traditional TA relies on predefined rules and visual patterns to guide trading decisions \cite{achelis2001}, machine learning offers a data-driven alternative capable of uncovering complex, non-linear relationships that may be difficult to formalize explicitly. In recent years, this has led to the development of hybrid systems that combine technical indicators with statistical learning models to improve predictive accuracy and robustness.

Nti et al.\ \cite{nti2020} provide a comprehensive systematic review of stock market prediction techniques, highlighting the transition from rule-based and econometric models to modern ML approaches. Their findings show that supervised learning, especially classification models for directional movement prediction (e.g., up or down), dominates the literature, with support vector machines (SVMs), decision trees, and ensemble methods such as random forests being widely adopted. At the same time, feature engineering remains closely tied to TA: most studies extract input variables from traditional indicators like RSI, MACD, and moving averages, showing that even in ML contexts, technical analysis continues to shape model design.

Deep learning has further expanded the boundaries of financial modeling, offering tools for both feature extraction and temporal dependency modeling. Olorunnimbe and Viktor \cite{olorunnimbe2023} survey deep learning applications in stock market prediction, noting the growing use of CNNs, LSTMs, and attention-based architectures. Their review emphasizes the importance of rigorous backtesting, hyperparameter optimization, and overfitting control—areas where deep learning has both introduced new capabilities and new challenges. Notably, they observe that while model complexity has increased, predictive performance gains remain context-specific, and there is no universally superior architecture.

Both reviews agree that a key difficulty in applying ML to financial time series lies in their inherent non-stationarity and high noise levels. Moreover, success is often sensitive to the choice of input features, target definition, and evaluation metrics. For instance, models trained to minimize mean squared error (MSE) may not align with trading objectives like risk-adjusted returns or drawdown minimization.

To bridge the gap between traditional TA and machine learning, some researchers have explored the use of technical indicators as feature encodings for deep models, while others have experimented with direct time-series representations such as candlestick charts or OHLCV sequences. Achelis \cite{achelis2001} provides a practical taxonomy of indicators and pattern recognition strategies, many of which serve as the foundation for modern input pipelines in ML-based trading systems.

Taken together, these developments reflect a shift in financial forecasting from manually engineered heuristics toward automated pattern discovery and learning. However, challenges such as data snooping, model interpretability, and real-world trading constraints remain open. These concerns have motivated the exploration of more advanced sequential models, most notably transformer-based architectures, which are introduced in the following section.


\section{Transformer Architectures}

Transformer architectures, introduced by Vaswani et al. (2017) in the seminal paper \textit{Attention Is All You Need}, have fundamentally reshaped sequential modeling, particularly in the field of natural language processing (NLP). Unlike recurrent neural networks (RNNs) and their variants such as long short-term memory (LSTM) models, which process sequences step-by-step, transformers operate on entire sequences in parallel. This parallelism enables significant gains in training efficiency and scalability. More importantly, transformers effectively model long-range dependencies through the mechanism of self-attention, overcoming the memory and gradient limitations of earlier recurrent architectures \cite{vaswani2017, radford2018}.

The core innovation of the transformer lies in its \textbf{attention mechanism}, particularly \textit{self-attention}, which allows the model to dynamically weigh the relevance of all elements in a sequence when processing any individual element. This mechanism underpins the model's ability to capture context-dependent and non-local relationships across sequences.

\begin{itemize}
    \item \textbf{Self-Attention}: Enables each token (or time step) to attend to every other position in the input sequence. This facilitates the modeling of complex dependencies regardless of sequence distance, which is essential in domains such as language understanding and financial forecasting.
    \item \textbf{Multi-Head Attention}: Extends self-attention by computing attention in parallel across multiple subspaces. Each ``head'' can focus on different types of relationships, allowing the model to simultaneously capture various levels of structure and interaction within the data.
    \item \textbf{Positional Encoding}: Since the transformer architecture does not impose any sequential ordering inherently, positional encodings are introduced to inject information about the relative or absolute positions of tokens. These encodings allow the model to retain a notion of temporal structure, which is critical in both text and time series data \cite{vaswani2017}.
\end{itemize}

Transformers have achieved state-of-the-art performance across a wide range of NLP tasks, including machine translation, question answering, summarization, and generative text modeling \cite{brown2020}. Their success is attributed to the capacity to model deep contextual relationships, capture latent structure in unstructured data, and scale effectively with large datasets and model sizes.

These capabilities are not unique to language. Financial time series, much like natural language, exhibit complex, non-linear dependencies, recurring motifs, and variable-length temporal structures. The non-local computation enabled by self-attention is particularly well-suited for identifying delayed or non-contiguous relationships - such as multi-day momentum, periodic reversals, or volatility regimes - that are often targeted by technical analysts \cite{zhang2023}.

Transformers can be trained in an \textbf{autoregressive} fashion, where the model predicts the next element in a sequence conditioned on all past elements:
\[
P(x_t \mid x_{<t}) = P(x_t \mid x_{t-1}, x_{t-2}, \dots, x_1)
\]
This formulation underlies many large language models (LLMs) such as GPT, which generate coherent sequences token-by-token \cite{radford2019}. In the context of financial modeling, this same autoregressive structure can be leveraged to predict future price values or directional movements based on prior historical data. It aligns conceptually with the logic of technical analysis, where traders infer future behavior from historical patterns.

Importantly, the transformers' ability to condition on arbitrarily long contexts, unlike traditional models with fixed memory windows, allows it to learn long-term trend evolution, sequential causality, and regime shifts. These capabilities are crucial for trading strategies like momentum detection, breakout forecasting, and volatility modeling \cite{zhou2021, lim2021}.

The combination of self-attention for pattern discovery and autoregressive modeling for sequential prediction positions transformers as highly promising for time series forecasting. Compared to classical approaches like ARIMA or even LSTMs, which are often constrained in their capacity to model long-term dependencies, transformers offer greater flexibility and representational power.

However, despite their transformative impact in NLP, the application of transformer architectures to financial forecasting, particularly in the context of technical analysis, remains relatively nascent. Time series data introduces unique challenges: non-stationarity, low signal-to-noise ratios, regime switching, and the lack of a natural ``vocabulary'' or semantic hierarchy typical in language data \cite{ni2024}.

This study seeks to explore whether transformer-based models can internalize the stylized facts, heuristics, and pattern-based reasoning that underpin technical trading. Specifically, it evaluates whether these models can capture the types of temporal dynamics that human traders use to identify support / resistance levels, momentum shifts, or volatility breakouts, assessing the practical viability of applying transformer architectures to model and predict structured behavior in financial markets.

\section{Transformers for Financial Time Series}

Although transformer architectures have become central in time-series forecasting research, their application to large-scale financial markets,particularly equity price prediction, remains limited. Much of the existing literature focuses on data domains such as weather, traffic, and energy systems, which are typically characterized by smoother dynamics, higher signal-to-noise ratios, and deterministic seasonal patterns. In contrast, financial time series are shaped by heterogeneous agents, reflexivity, and rapid feedback loops, leading to non-stationarity, regime shifts, and substantial noise. These characteristics make financial forecasting both more complex and less predictable than many natural processes.

A defining feature of financial markets is their sensitivity to human behavior. Unlike physical systems governed by stable laws, markets are influenced by collective psychology, sentiment, and institutional incentives. Technical analysis (TA) builds directly on this premise, asserting that price movements reflect recurring behavioral patterns, such as momentum, support / resistance levels, and volatility breakouts that can be identified and leveraged for trading decisions. This assumption distinguishes financial time series from those in natural sciences governed by mechanistic causality. Accordingly, applying transformer-based models - originally designed for symbolic and language data - to financial forecasting requires addressing fundamentally different sources of structure: ones driven not by physical laws but by cognitive and strategic behavior.

Despite these challenges, transformer models offer appealing properties for modeling financial time series. Their ability to capture long-range dependencies, model temporal context flexibly, and scale to high-dimensional inputs makes them a promising candidate for learning patterns in complex market environments governed by human behavior. 

In recent literature, three major research directions have emerged around applying transformers in time-series forecasting: (1) the use of LLMs for zero-shot forecasting; (2) architecture-level innovations for time-series adaptation; and (3) benchmarking studies evaluating their empirical competitiveness.

Gruver et al. \cite{gruver2023} show that instruction-tuned LLMs can perform zero-shot time-series forecasting directly from raw numeric sequences. Without any gradient-based fine-tuning, models like GPT-3.5 produce competitive one-step and multi-step forecasts across various domains, including equity indices and currency rates. The underlying ability is attributed to two key features: the autoregressive training objective, which implicitly teaches next-value prediction, and the model's encoded knowledge of real-world structures acquired during pre-training. While LLMs generally underperform domain-specific models on raw error metrics, they offer substantial practical benefits - most notably, the ability to make forecasts without training data. In the financial context, this opens up possibilities for rapid prototyping, exploratory scenario analysis, and priors for Bayesian-style pipelines.

At the architectural level, transformer variants have been tailored specifically for long-horizon time-series forecasting. Liu et al.\ \cite{liu2023itransformer} propose iTransformer, which introduces an inverted attention mechanism where queries attend to groups of keys and values extracted from non-overlapping patches of the input. This not only reduces computational complexity from $\mathcal{O}(L^2)$ to $\mathcal{O}(L\,p^{-1})$, where $p$ is the patch length, but also improves empirical performance on long-range benchmarks such as ETTm1. Similarly, earlier work by Nie et al.\ (PatchTST) demonstrates that simply dividing the input into patches and removing positional encodings can outperform more intricate attention sparsification methods. Other models, such as Autoformer and FEDformer, decompose time series into trend and seasonal components before applying attention, making them particularly relevant for financial assets that exhibit periodic structure, such as foreign exchange and commodities. These innovations collectively highlight the importance of domain-aware tokenization and sparsity in adapting transformers for forecasting financial data.

However, Zeng et al.\ \cite{zeng2023} provide a sobering counterpoint, emphasizing that transformers are not universally superior. Their extensive benchmarking across eight public datasets reveals that, once proper normalization and scaling are applied, simple linear models or shallow multilayer perceptrons (MLPs) often outperform transformer variants. Their findings suggest that the key to good forecasting performance may lie more in pre-processing and data treatment than in model complexity. This is especially pertinent in finance, where low signal-to-noise ratios and frequent regime changes can cause overfitting in deep models. These insights serve as a caution against assuming that transformers are always the optimal choice, reinforcing the need for rigorous backtesting and economic validation, particularly when applying such models to trading strategies derived from technical analysis.

Despite the progress, several open challenges remain. First, financial markets are inherently non-stationary, and transformers trained under the assumption of stationarity may struggle during regime shifts unless explicitly adapted through online learning or meta-learning strategies. Second, most evaluation metrics focus on statistical loss functions such as MAE or MSE, while neglecting financially meaningful criteria like Sharpe ratio, maximum drawdown, or transaction-cost-adjusted returns. Third, data scarcity continues to be a constraint, especially in emerging markets or niche asset classes where long and clean historical series are unavailable. While synthetic data and transfer learning offer potential solutions, they remain underexplored in the financial context.

While transformers can learn from non-stationary data, structural changes in financial time series, such as regime shifts and volatility spikes, still challenge generalization, especially in zero-shot settings. Moreover, technical indicators often assume stationary distributions, and their interpretations may break down when the data evolves. For robust performance, attention to temporal segmentation and data regimes remains essential.

\chapter{Empirical testing (plan)}

This thesis investigates the forecasting capabilities of transformer-based models in financial time series by comparing two approaches:

\begin{enumerate}
    \item \textbf{Zero-shot inference} using large language models (LLMs) deployed locally on CSC infrastructure
    \item \textbf{A supervised domain-specific transformer}, trained from scratch on historical price data
\end{enumerate}

The study focuses on the task of predicting short-term trading signals (e.g. Buy / Hold / Sell) from recent stock price movements, with consistent evaluation metrics across both modeling paradigms.

\section{Task formulation}

The predictive task is modeled as a classification problem based on future log-returns. The target labels are created by discretizing the return distribution. Two alternative schemes could / will be considered to enable detection of more robust price swings:

\subsubsection*{Option A: 3-Class Classification with Higher Thresholds}
\begin{itemize}
    \item \textbf{Buy}: Future return $\geq +3\%$
    \item \textbf{Sell}: Future return $\leq -3\%$
    \item \textbf{Hold}: Otherwise
\end{itemize}

This setup emphasizes high-confidence and strong-signal predictions. It simplifies modeling but may increase class imbalance due to fewer extreme movements.

\subsubsection*{Option B: 5-Class Classification (Granular Signal Tiers)}
\begin{itemize}
    \item \textbf{Strong Buy}: Future return $\geq +5\%$
    \item \textbf{Buy}: $+1\%$ to $+5\%$
    \item \textbf{Hold}: $-1\%$ to $+1\%$
    \item \textbf{Sell}: $-5\%$ to $-1\%$
    \item \textbf{Strong Sell}: Future return $\leq -5\%$
\end{itemize}

This approach supports more nuanced signal calibration and enables analysis of model expressiveness across volatility levels, but may require more data and careful handling of class imbalance.

Both labeling schemes will be evaluated using consistent input-output structures across the supervised and zero-shot models.

\section{Input Features}

Models are tested in two phases to assess the contribution of different feature sets:

\subsection*{Phase 1: Price-only Inputs}
\begin{itemize}
    \item Input: 3--10 recent daily closing prices (see \cite{gruver2023})
    \item Supervised model: uses normalized log-return vectors
    \item Zero-shot LLM: receives prices embedded in a natural language prompt
\end{itemize}

\subsection*{Phase 2: Feature-Augmented Inputs}
Additional market variables are included to test the effect of feature enrichment:
\begin{itemize}
    \item Daily trading volume
    \item Technical indicators (e.g., RSI, MACD)
\end{itemize}

In the supervised model, these are added as extra features. For LLMs, they are injected into the prompt as natural language, e.g.:
\begin{quote}
\texttt{Volume: 31.2M} \\
\texttt{RSI: 67.4} \\
\texttt{MACD: +0.42}
\end{quote}

This design enables direct comparison of model behavior under sparse vs. enriched feature conditions.

What about supervised model?? Arrays, which in prices-only mode are half empty, or re-train the whole thing?? Talk with Teemu Roos!

\section{Zero-Shot Inference Using LLMs}

Zero-shot experiments are conducted using locally deployed open-source models (e.g., LLaMA 2, Mistral) on CSC Puhti supercomputer. Prompts follow this format:

\begin{quote}
Here are the closing prices for Company X: \\
2024-06-17: \$194.50 \\
2024-06-18: \$197.80 \\
2024-06-19: \$199.10 \\
2024-06-20: \$198.20 \\
2024-06-21: \$200.35 \\

Based on this trend, what is the next trading action? \\
A. Buy \\
B. Hold \\
C. Sell \\

Answer:
\end{quote}

For 5-class classification, options are extended to include \texttt{Strong Buy} and \texttt{Strong Sell}.

\textbf{Output Mapping:} Model completions are mapped to class labels based on exact match or soft regex matching.

\textbf{Confidence Estimation:} When logits are accessible, softmax or entropy is used; otherwise, prompt sampling with temperature control or ensemble prompting approximates uncertainty.

\subsection{Supervised Transformer Model}

A decoder-only transformer (NanoGPT-scale) is trained on the same data:

\begin{itemize}
    \item Input: Normalized return sequences with optional feature augmentation
    \item Output: Softmax over 3 or 5 classes depending on the labeling scheme
    \item Loss: Cross-entropy
\end{itemize}

This model provides natively calibrated probabilities and serves as a domain-specific baseline.

\section{Evaluation Metrics}

\subsection*{Classification Metrics}
\begin{itemize}
    \item Accuracy
    \item Precision, Recall, F1-score (macro + per-class)
    \item Confusion matrix
\end{itemize}

\subsection*{Confidence-Aware Metrics}
\begin{itemize}
    \item Accuracy by confidence decile
    \item Top-$k$ accuracy (where applicable)
    \item Expected Calibration Error (ECE)
\end{itemize}

\subsection*{Financial Metrics (Optional)}
\begin{itemize}
    \item Cumulative return (from backtested signals)
    \item Sharpe ratio
    \item Maximum drawdown
\end{itemize}

\section{Infrastructure and Tooling}

Experiments are run on the CSC Puhti supercomputer:

\begin{itemize}
    \item \textbf{Supervised model training}: PyTorch / PyTorch Lightning
    \item \textbf{LLM inference}: Hugging Face Transformers, \texttt{llama.cpp}, text-generation-webui
    \item \textbf{Data and metrics}: pandas, NumPy, scikit-learn, matplotlib
\end{itemize}

API-based LLM access is avoided entirely to eliminate cost and latency overhead. All models run locally, enabling reproducibility and access to internal logits where possible.

\section{Summary Table}

\begin{table}
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Aspect} & \textbf{Supervised Transformer} & \textbf{Zero-Shot LLM} \\\hline
Training & Supervised & None \\\hline
Input & Normalized returns (+features) & Natural language prompt \\\hline
Label Space & 3 or 5 classes & 3 or 5 text labels \\\hline
Confidence & Softmax score & Logits or sampling \\\hline
Compute & CSC GPU training & CSC LLM inference \\\hline
API Cost & None & None \\\hline
Evaluation & Classification + financial & Same \\\hline
\end{tabular}
\caption{Comparison of modeling approaches}
\end{table}



\chapter{Figures and Tables}

\section{Figures}
Figure~\ref{fig:logo} gives an example how to add figures to the document. Remember always to cite the figure in the main text. There are many ways to cite, for example: University of Helsinki has a nice logo (see Fig.~\ref{fig:logo}).
\begin{figure}[h!] 
\centering 
\includegraphics[width=0.3\textwidth]{UH-logo.png}
\caption{University of Helsinki flame-logo for Faculty of Science.\label{fig:logo}}
\end{figure}

\section{Tables}

Table~\ref{table:results} gives an example how to report experimental results. Remember always to cite the table in the main text. There are many ways to cite, for example: The results are as expected (see Table~\ref{table:results}).

\begin{table}
\centering
\caption{Experimental results.\label{table:results}}
\begin{tabular}{l||l c r} 
Koe & 1 & 2 & 3 \\ 
\hline \hline 
$A$ & 2.5 & 4.7 & -11 \\
$B$ & 8.0 & -3.7 & 12.6 \\
$A+B$ & 10.5 & 1.0 & 1.6 \\
\hline
%
\end{tabular}
\end{table}

\chapter{Citations}

\section{Citations to literature}

References are listed in a separate .bib-file. In this case it is named \texttt{bibliography.bib} with the following content:
\begin{verbatim}
@article{einstein,
    author =       "Albert Einstein",
    title =        "{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
        [{On} the electrodynamics of moving bodies]",
    journal =      "Annalen der Physik",
    volume =       "322",
    number =       "10",
    pages =        "891--921",
    year =         "1905",
    DOI =          "http://dx.doi.org/10.1002/andp.19053221004"
}
 
@book{latexcompanion,
    author    = "Michel Goossens and Frank Mittelbach and Alexander Samarin",
    title     = "The \LaTeX\ Companion",
    year      = "1993",
    publisher = "Addison-Wesley",
    address   = "Reading, Massachusetts"
}
 
@misc{knuthwebsite,
    author    = "Donald Knuth",
    title     = "Knuth: Computers and Typesetting",
    url       = "http://www-cs-faculty.stanford.edu/%7Eknuth/abcde.html"
}
\end{verbatim}

In the last reference url field the code \verb+%7E+ will translate into \verb+~+ once clicked in the final pdf.

References are created using command \texttt{\textbackslash cite\{einstein\}}, showing as \cite{einstein}. Other examples: \cite{latexcompanion,knuthwebsite}.

Citations should be arranged in alphabetical order by author, using the default style \texttt{abbrv}.



\section{Crossreferences}

Appendix~\ref{appendix:code} on page~\pageref{appendix:code} contains a code example.

\chapter{From tex to pdf}

In Linux, run \texttt{pdflatex filename.tex} and \texttt{bibtex
  filename} repeatedly until no more warnings are shown. You should
use \texttt{pdflatex} when compiling your document.
 
\chapter{Conclusions\label{chapter:conclusions}}

It is good to conclude with some insightful discussion. 

% STEP 5:
% Uncomment the following lines and set your .bib file and desired bibliography style
% to make a bibliography with BibTeX.
% Alternatively you can use the thebibliography environment if you want to add all
% references by hand.

\cleardoublepage %fixes the position of bibliography in bookmarks
\phantomsection

\addcontentsline{toc}{chapter}{\bibname} % This lines adds the bibliography to the ToC
\bibliographystyle{abbrv} % numbering alphabetic order
\bibliography{bibliography}

\begin{appendices}
\myappendixtitle

\chapter{Code example\label{appendix:code}}
Program code can be added as appendix:
\begin{verbatim}
#!/bin/bash          
text="Hello World!"
echo $text
\end{verbatim}

\end{appendices}

\end{document}
